# インフラ

インフラの設計思想を記述する。
2重起動はしないようにする
自動復旧する

---

## インフラサーバの潮流

### 物理サーバーの時代

インフラといえば、アプリケーションがどれくらい利用されるか予測し**物理サーバを業者に発注**していた。（サーバー調達と言います）
メーカーの都合でハードウェアや仕様が変わったり、求めるサーバーが手に入らないことがあることある。
負荷対策では強いマシンを用意したり、ロードバランサーと複数台のマシンで分散処理します。

**複数サーバーへのデプロイは？**
サーバーが少数台であれば、1台1台にアプリケーションコードを配置（デプロイメント）するのは難しくありません。

rsyncなどのツールを使って効率的にコードを配置していた（昔やっていた）

しかし、サーバー台数が多くなってきたり、さまざまなサーバー(仕様やOS)が混在するようになると難度は上がっていきます。セットアップにミスがあると、サーバーによってはPythonが入っていないだとか、バージョンが古いだとか、トラブルを引き起こします。
**こうしたサーバ環境を管理することをプロビジョニング**と言う。
プロビジョニングを行うためPuppet、Ansible、Chefがそれらツールの代表。
サーバーに一括で同じスクリプトを実行することが簡単になった。

### 仮想化技術時代

データセンターでは、サーバー仮想化が普及することになります。
1台のハイスペックなマシンを、複数台の仮想サーバとして分割するための技術です。

ホストマシンの中でOSとカーネルを仮想化し、ユーザにアプリケーション実行環境を公開します。
レンタルサーバの裏側で使われている技術です。

この仮想化技術を使って大量のサーバを保持する会社が、指定したサーバスペックを短時間で調達するIaaSが生まれました。仮想化によってサーバ環境の統一やプロビジョニングが容易になりました。

## Dockerの誕生

さきほど述べた仮想化技術はスーパーバイザー型、ホスト型などと呼ばれ、ホストマシン上でゲストマシンを立ち上げ、OSやカーネル、アプリケーションを動かします。

これらの技術を使うと、サーバ調達は短時間で簡単に済みます。とはいえ、もっと素早いリソースの調整が求められていました。アプリケーションが主体になっていく流れの中でOSやカーネルまで仮想化が必要なのかという疑問が生まれました。

アプリケーション動作環境をうまく仮想化するだけの技術としてDockerが生まれました。
DockerはホストマシンのOSと**カーネルを共有**し、アプリケーションだけを動作させます。

OSやカーネルを仮想化しない分、短時間で起動が可能です。いまではLinuxディストリビューションに限らずWindowやmacOSなどで使えるような仕組みに変わっています。

コンテナーオーケストレーション
Dockerはコンテナーと呼ばれる隔離環境でアプリケーションを実行します。1台のサーバ上でお互いのプロセスに影響を与えることなく、複数のコンテナーを動かすことが可能になります。

となると、どのサーバーにどのコンテナーを動かすのかという問題が生じます。これを解決するのが、コンテナーオーケストレーションツールです。KubernetesやApacheMesosがそれらにあたります。ApacheMesosはDocker以前から存在していました。

このようにして簡単に生成・破棄できるコンテナー型仮想化技術が重宝され、新しくオーケストレーションの問題が生まれ、また新しくKubernetesのようなツールで解決が図られているという流れです。

## インフラ歴史２

昔は各環境毎に設定手順書やスクリプトを手書きして、サーバを構成することが主流
しかし、設定手順書やスクリプトを実際の環境に合わせて適切に更新し運用し続けることにはかなりの負荷を伴う。

- 自動化ツール時代
その後、運用負荷の問題を改善するため `Chef` や `Itamae` や `Ansible` といったサーバを自動構成するためのDSLを提供するツールを利用するようになった。
これらのツールを使って設定を更新することで、設定変更が常にリポジトリにあるコードベースと一致することになる。
このようにコードを用いて実際の環境との不一致を解消し、差分管理のしやすいインフラ構成を実現するという流れをInfrastructure as Codeと呼ぶ。」
しかし自動構成するためのDSLは基本的に冪等になるように記述しなければいけない制約があり、柔軟な構成を実現するためにある程度の工夫が必要になる課題があった。

- 完成
この流れで登場したのがコンテナー、コンテナー技術自体は昔からUnixやLinuxで利用可能な技術だがインフラ構成にそのまま利用するにはかなり工夫が必要であり、その状況を大きく変化させたのがDocker。
Dockerによってアプリケーションやミドルウェアの動作環境を丸ごとパッケージとして提供したり、1つのOSの中で複数のコンポーネントを動作させるのが楽になった。
